{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115e48f1",
   "metadata": {},
   "source": [
    "# Train and Test Data Split\n",
    "In this notebook we will work on the train and test data split. It might look simple at first, what can be so difficult with assigning the train and test data, right? However, as we say, the further you go into forest, the more trees there are.\n",
    "\n",
    "When it comes to splitting the data we need to remember about a few rules that will allow us to create better models and allow for more precise model evaluation. \n",
    "\n",
    "**Basically, why do we split the data into train and test sets?**  \n",
    "In theory - we don't have to do it to create a model. We just need to feed the model with the independent data (our **X** values, all our features) and dependent data (our **y**, our desired outcomes) and our model will learn how to predict the y for the data we input later. But will it be a good model? Probably not. Will we know how good or bad it is? Also, probably not.   \n",
    "\n",
    "If we however divide the data into train and test datasets, we will, as the names suggest, train the model on the training data and then do some testing on the test data. Then we can compare the outcome on the test data with the actuall results and therefore evaluate our model. If the model does poorly on the predictions, it's a signal for us that we might want to tweek it a bit, change the parameters or do some more feature engineering so that we get a better result. We can also see, that the model is under- or overfitted and therefore it will make mistake on new data. \n",
    "\n",
    "In this notebook we will take a more theoretical look at the data splitting and some of the issues that come either from the splitting, but also take a look at the most common issues that might pop up at this point of machine learning journey:\n",
    "1. Using a population or sample data\n",
    "2. Basic train/test split\n",
    "3. Validation methods\n",
    "4. Dealing with imbalanced data\n",
    "5. Information Value * (I will add it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686509b",
   "metadata": {},
   "source": [
    "## Using population or sample data\n",
    "The dataset we are using could also be called our population of data. Depending on the size of this dataset we might notice, that the population is considerably large (to consider if we have more than few hundred thousand records) and takes up a lot of memory, and therefore leads to longer modeling, especially when we are just experimenting, searching for correct model or trying to optimize the results. In such scenario we might consider using a sample of the data instead of the whole populations. \n",
    "\n",
    "|  | **Advantages** | **Disadvantages** |\n",
    "|:---|:---:|:---:|\n",
    "| **Population** | Represents the tendencies for the whole population | Takes more time to be processed, <br>Assumes that we have all the data, <br> Might be imbalanced for minority classes |\n",
    "| **Sample** | Is faster to process and model, <br>Might help with imbalanced data | Provides a generalization of the populations tendencies, <br>Might be bias or be wrongly sampled |\n",
    "\n",
    "Choosing if we want to work on a sample or populations depends on different factors, dataset size, resources and time we have to do the task. If we want do use a sample, we can go a few different routes to achieve the desired, best sample for our task:\n",
    "- **Simple random sample** - which will create a random sample of the size we desire from our dataset. These data will we totally random and might miss on some patterns, therefore choosing the right size is very importand here, as well as comparing the sample to population for similarities;\n",
    "- **Stratified sample** - in which we divide the population into subgroups for the unique values (layers) of a feature and draw a sample from it. It ensures that every layer is represented, even it the proportions are slightly off.\n",
    "- **Proportional sample** - in which we are trying to save the proportions of the of the given feature;\n",
    "- **Systematic sample** - in which we take sample given the some intervals. We use a random starting point and then take n record in steps of our intervals. This works best if the data is sorted by some feature.\n",
    "\n",
    "> For our machine learning in these notes we do not need to take a sample, the dataset has only ~50k records and therefore is quite small, but we will draw the explained samples for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414a8ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>cube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.202030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.505856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.076885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46.724580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51.917250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  cut  clarity  depth  table  price  E  F  G  H  I  J       cube\n",
       "0   0.23    2        3   61.5   55.0    326  1  0  0  0  0  0  38.202030\n",
       "1   0.21    3        2   59.8   61.0    326  1  0  0  0  0  0  34.505856\n",
       "2   0.23    1        4   56.9   65.0    327  1  0  0  0  0  0  38.076885\n",
       "3   0.29    3        5   62.4   58.0    334  0  0  0  0  1  0  46.724580\n",
       "4   0.31    1        3   63.3   58.0    335  0  0  0  0  0  1  51.917250"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's import our libraries and read in our dataset that we preprocessed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# read our dataset for this notebook\n",
    "df = pd.read_csv('Diamonds_encoded.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4ed934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple random sample\n",
    "\n",
    "random_sample = df.sample(n=5000, # here we input the size of our sample\n",
    "                         random_state=42 # random state ensures that each time we draw, the sample will remain the same\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e74e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportional sample on clarity\n",
    "clarity_dict = {0: 'I1', 1: 'IF', 2: 'SI1', 3: 'SI2', 4: 'VS1', 5: 'VS2', 6: 'VVS1', 7: 'VVS2'} #as reminder\n",
    "\n",
    "# let's set up the weights \n",
    "# if we want the data to be splitted exactly, all should have the same weight assigned\n",
    "weights = {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1} \n",
    "\n",
    "proportional_sample = df.sample(n=5000, \n",
    "                         random_state=42,\n",
    "                         weights=df['clarity'].map(weights) # assigning weights\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c4fcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sample on cut\n",
    "\n",
    "proportion = 0.1  # Sample size as proportion to the population, here 10%\n",
    "\n",
    "stratified_sample = df.groupby('cut').apply(lambda x: x.sample(frac=proportion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7822fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# systematic sample with sorted cut\n",
    "df1 = df.sort_values(by='cut') # we need to sort and I don't want to overwrite my df\n",
    "\n",
    "# the following part can be done in one line as well\n",
    "proportion = 0.1\n",
    "population_size = len(df1)\n",
    "sample_size = int(population_size * proportion) \n",
    "step = population_size//sample_size\n",
    "\n",
    "# assign first draft\n",
    "first_index = np.random.randint(0, sample_size) \n",
    "\n",
    "systematic_sample = df1[first_index::step].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b0337",
   "metadata": {},
   "source": [
    "When it comes to sampling, these would be the basic sampling methods to use if we were to resample our dataset, if it is to big or we would like to first play with our data in a way, that involves less computing.\n",
    "\n",
    "There are also different techniques in `scipy.stats` or in `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1aabd",
   "metadata": {},
   "source": [
    "## Basic train and test split\n",
    "Now that we decided whether we want to work on the population or sample, we can split our data for our machine learning model. As mentioned before, we do it so that we can check how well or bad our model is behaving. In many situations we will be spliting not only into training and testing sets, we will also create a validation set for hyperparameters tuning, but more on that later.\n",
    "\n",
    "Most often, we will split our data into 70-80 % for training and 20-30% for testing (and validation). For larger datasets we can split it more deliberately, for example 50/50 or 90/10, but we can dive into that the more we understand and tune our model. \n",
    "\n",
    "As the names sugest, we use our **training set** to train our model, providing it with the features/parameters values X and the expected values y. The **testing set** serves as, obviously, a test, in which we compare the model prediction with the expected, correct values. The more our model is correct in it's estimation, the better. \n",
    "\n",
    "> **In this notebook we will be using cut as the dependent value.**  If we were modeling at this point, after the train/test split we might do scaling on our X_train and later scale X_test when predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e3d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import our needed methods\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc117cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now assign the dependent and independed variables\n",
    "y = df.pop('cut') #dependent variable\n",
    "X = df # independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d56101",
   "metadata": {},
   "source": [
    "Of course for the X/y split there are tons of methods, but this is the one I like right now. If we are choosing only a few columns from the whole dataset for prediction, this code could look like this:\n",
    "```python\n",
    "X = df[['cut', 'depth', 'cube', 'clarity']]\n",
    "y = df['price']\n",
    "```\n",
    "\n",
    "We could also create pipelines that will automaticly assign the classes/expected values. This might come in handy when updating and already existing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ec6ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43152, 12) (10788, 12)\n",
      "(43152,) (10788,)\n"
     ]
    }
   ],
   "source": [
    "# now let's split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, # our variables\n",
    "                                                   test_size=0.2, # proportion of the test size\n",
    "                                                   random_state=42, # so it is always divides same way\n",
    "                                                   stratify=y # for classification, so we get equal number of classes\n",
    "                                                   )\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82bbaa",
   "metadata": {},
   "source": [
    "## Validation and it's methods\n",
    "In the previous part, I mentioned that we can also distinguish a validation set. But before that, let's consider, what actually is validation and why we need it, when creating a machine learning model. \n",
    "\n",
    "If we were to split our data into train-validation-test, the validation set would be there to evaluate our model. Sounds too similar to what testing set does? The difference we make in here is that we keep the testing set until the very end, so that we could evaluate the model on data it hasn't seen before. Machine learning models tend to learn the provided data *by heart*, so to speak, therefore it's good to evaluate on data it has never seen before. The validation set is there, so that we can evaluate the model many times before calling it perfected (or good enough) and stop tuning the hyperparameters. Then, if the results are same or close on the testing set, we are good to go with deploying our model.\n",
    "\n",
    "There is also an alternative for validation set called **cross-validation**. It is useful, since if we were to use a validation set, we would have to sacrifice approximatel 10-20% of our data, that could be used for training. With cross-validation, the data can stay in the training set and we will have a way to validate the model. \n",
    "\n",
    "Cross-validation also serves as a controlling step, that will lessen the likelihood of model ovefitting. By using cross-validation, the data we feed into our model will be given in different states, shuffled and therefore our model will be more robust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca58f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic way with validation set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify=y \n",
    "                                                   )\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, # we use train sets in here!\n",
    "                                                  test_size=0.25,  # remember that X_train = 0.8 of X\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_train\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2e3fe",
   "metadata": {},
   "source": [
    "Now with that we would:\n",
    "- use training set to train the model;\n",
    "- use validation set to validate the model and then tune the hyperparameters to see, if we can create a better outcome;\n",
    "- use the test set to test the final model after validation.\n",
    "\n",
    "However this way we are training the model on only **60%** of data. Therefore we call for the cross-validation method to the rescue.\n",
    "\n",
    "What the most of cross-validations do is they create a desired number of folds of train/test sets, there test data don't overlap in different folds. With these folds, we can train our model for the number of folds and take the average score, on which we are evaluating the model (more on the evaluation in different notebook!). When we are training the model with cross-validation, we do it in a loop.\n",
    "\n",
    "There are quite a few methods on cross-validation from Sklearn and we will take a few into consideration here, with honorable mentions as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09593b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the cross-validation methods\n",
    "from sklearn.model_selection import (LeaveOneOut, \n",
    "                                     KFold, \n",
    "                                     StratifiedKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1d526",
   "metadata": {},
   "source": [
    "### Leave-One-Out\n",
    "Leave-One-Out is one of the cross-validation methods, that can be used to teach a ML model. In each loop, the algoritm will leave one record out and use it for evaluation, hence the name.  \n",
    "![LOOCV](https://www.bijenpatel.com/content/images/2020/11/loocv.png)\n",
    "\n",
    "LOO cross-validation is however very computationally expensive, as it basicly loops through the whole given dataset. It is therefore not recommended as a validation method for larger datasets, as it might simply take to long to be computed. LOO is a great method for smaller datasets (up to few thousand records) and might be great solution for fighting the overfitting, as it can be very precise.\n",
    "> In this notebook I will code the LOO for the diamonds dataset but won't run it, since it is a larger dataset.\n",
    "\n",
    "Alternatively to LOO, there is another cross-validation method called `LeavePOut()`, which allows us to define the number of records left out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12775fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv = LeaveOneOut()\n",
    "\n",
    "for train, test in loocv.split(X, y): # this loop provides us with the indexes of train and test rows\n",
    "    X_train, y_train = X.iloc[train], y.iloc[train] # assign the train spilt\n",
    "    X_test, y_test = X.iloc[test], y.iloc[test] # assign the train split\n",
    "    \n",
    "    print(X_train.shape, X_test.shape) # here we will follow up with model fitting in next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13ee71",
   "metadata": {},
   "source": [
    "### KFold\n",
    "KFold cross-validation method might be the most popular one, as it's much faster and computationally cheaper. Similarely to Leave-One-Out, in each loop it takes a piece of the data for testing and does it for the k-number of folds. The testing data does not overlap and is evenly distributed in all the folds.\n",
    "![KFold](https://i0.wp.com/sqlrelease.com/wp-content/uploads/2021/07/K-fold-cross-validation-1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9570a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n"
     ]
    }
   ],
   "source": [
    "kfoldcv = KFold(n_splits=5, # number of folds, 5 is default\n",
    "               shuffle=True, # if true, the data is shuffled at the beggining\n",
    "               random_state=42 # ensures that when we use it again we will get same results\n",
    "               )\n",
    "\n",
    "# and the loop is the same for each cross-validation\n",
    "for train, test in kfoldcv.split(X, y):\n",
    "    X_train, y_train = X.iloc[train], y.iloc[train]\n",
    "    X_test, y_test = X.iloc[test], y.iloc[test]\n",
    "    print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf7bca",
   "metadata": {},
   "source": [
    "### Stratified KFold\n",
    "This cross-validation is a variation of KFold that returns stratified folds (we talked about stratification earlier). The folds are made by preserving the percentage of samples for each category. This is a good cross-validation for imbalanced data, as it ensures the presence of each category in every training and testing fold.\n",
    "![Stratified KFold](https://i.stack.imgur.com/XJZve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07c24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n"
     ]
    }
   ],
   "source": [
    "strkfoldcv = KFold(n_splits=5,\n",
    "               shuffle=True,\n",
    "               random_state=42 \n",
    "               )\n",
    "\n",
    "# and the loop is the same for each cross-validation\n",
    "for train, test in strkfoldcv.split(X, y):\n",
    "    X_train, y_train = X.iloc[train], y.iloc[train]\n",
    "    X_test, y_test = X.iloc[test], y.iloc[test]\n",
    "    print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee987d8",
   "metadata": {},
   "source": [
    "### Groups\n",
    "With cross-validation and generally, in data analytics and ML as well, we sometimes come across the usage of *groups* in our datasets. What are these and how do we use it? Well, I'm glad I asked. 😁\n",
    "\n",
    "In regards to our dependent and independent variables, groups won't fit into any of these. The groups are there in the dataset as the sources of the records. \n",
    "\n",
    "For example, let's assume that for the dataset we are working with, diamonds, each unique diamond color comes from a different mine. And therefore, if we are doing a classification for the cut of the diamond, we can later see, where do the best cuts come frome and where the cuts are so-so (this is a made up example to the used dataset, as unfortunately there are not groups in it).  \n",
    "Another example could be that we are given the patients data from few different hospitals and need to determine, if they are going to have a heart attack or not. The group in here would be the hospitals, from which the informations about the patient come from. If one of the hospital has high level of heart attacks, maybe they do not take good care of their patients? Or maybe this hospital is located in an unhealthy area? Or, perhaps, they specialize in heart diseases? \n",
    "\n",
    "Either way, different sources of information/records might cause the model to be to sensitive or not sensitive enough for our prediction. Like in the example given above with the hospitals, if we give the model training data from \"normal\" hospitals and then ask the same model to predict the heart attacks for patients in the heart-specializing hospital, we won't get good predictions. This might also go the other way round, as if we would train the model with data from mainly heart-spetializing hospitals and then ask it to predict heart attacks for a childrens hospital, where the rates are much lower.\n",
    "\n",
    "Importatant to remember here is that groups aren't a feature in the sense that we train our model with them. We most likely will come across the groups when creating a larger, general models, in which we would like to avoid biases and make mistakes based on the sources of our data, if they come from a source different than the ones we trained our model with.\n",
    "\n",
    "Why mentioning it? Well, there are cross-validation methods that allow us to validated our train and test data with considerations of the groups as well.\n",
    "\n",
    "### Honorable mentions\n",
    "As I explained, how the groups work in data above, we can move on to a few different cross-validation methods that are involving the usage of groups.\n",
    "- **GroupKFold** - which ensures that for each fold our model is validated on data from a different group(s) than the ones it was learned on;\n",
    "- **StratifiedKFold** - which is a mixture of it's name, it takes data from each group to be trained and tested on, while preserving the percentage of samples for each group;\n",
    "- **LeaveOneGroupOut** - where for each iteration, different group is left out for validation;\n",
    "- **PredefinedSplit** - in which we predefine the schema for the spilt;\n",
    "- **TimeSeriesSplit** - which is a special cross-validation method for time-series forecasting, which I will coved later on, in time series notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5155c3",
   "metadata": {},
   "source": [
    "## Dealing with imbalanced data\n",
    "Imbalanced data is a dreadful situation to think of, but it's farely common, when dealing with minory classes in our data or trying to predict something rare, like shark attacks. The less likely or less common the cagetory is, the harder it will be for our model to differentiate it from the majority category(-ies). What is also important to note here is that usually those minority classes are the once that are crutial to us and important to recognize. For example, there is less people that are pre-diabetic than the once that are not, but it is more important the recognize the ones that are than the ones that aren't, since with proper predictions we might save some people from being diabetic.\n",
    "\n",
    "We talk about imbalanced data when in a binary classification one of our categories is seen in less than 10% of record or lower. The margin might be sligly higher or lower, depending on the context, but when it goes below 10%, we might debate whether we are dealing with imbalanced data or not. A golden rule would say, that the bigger is the difference between the smallest class and the rest the more trouble will there be for our model. If our minority class takes less than 10% of the data, then if we would test our y_test with an array of just the values of the majority class, we still would get at least 90% of accuracy, which in itself might look not bad, but it doesn't serve the purpose of our model, to detect these 10% of minorities. \n",
    "\n",
    "In case we are dealing with imbalance, if we want our model to be sensitive for minority classes and be able to predict them, we have a few solutions to handle that.\n",
    "1. We might use sampling to try to even out the proportions of our classes;\n",
    "2. Stratified cross-validation helps us with balancing the data and not skipping over minory class(es);\n",
    "3. We can use Random Oversampling or SMOTE (Synthetic Minority Oversampling Technique);\n",
    "4. We can also do Random Undersampling or TomekLinks on the majority class;\n",
    "5. We can use an ML algoritm that uses weight-classes and assign higher weights to the minority class(es)\n",
    "\n",
    "We already discussed sampling and cross-validation and we will talk about models in the next notebook, hence we will now move on to the over- and undersamplin and SMOTE. However, these techniques provide best results when used on binary classification and in lower dimentionalities. Our diamonds dataset is rather low in dimentions, but for our predicted value *cut* we have 5 classes. Therefore in this notebook I will discuss these techniques on titanic dataset with only numeric columns for the sake of these examples.\n",
    "\n",
    "> 💡 **Important! When we are balancing the data for the model, we only do so with training data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91f9ede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived\n",
       "0    0.593838\n",
       "1    0.406162\n",
       "Name: perc, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the titanic for our balancing examples\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic = titanic[['survived', 'age', 'sibsp', 'parch', 'pclass', 'fare']]\n",
    "titanic = titanic.dropna(axis=0)\n",
    "# I narrower the dataset as I don't want to waste time encoding these values\n",
    "\n",
    "# let's see if the survived data is imbalanced\n",
    "a = titanic.groupby('survived').count()\n",
    "a['perc'] = a['fare']/a['fare'].sum()\n",
    "a['perc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16842add",
   "metadata": {},
   "source": [
    "The hasn't survived/survived ratio is actually not that bad, we wouldn't have to balance it out, as the data is well represented in both classes. But of course, we will do it for the sake of these examples and explanations and since I am slightly too lazy to search for appropriate dataset 😝. Generally, in cases where the data are not balanced 1:1, but both (or all) categories are well represened in the dataset it's better not to use these methods, as it also might disrupt the ditribution patterns.\n",
    "\n",
    "## Balancing the dataset with binary classes\n",
    "### Random Oversampling and Undersampling\n",
    "I have put these two methods together, as these use the same methodology, but in opposite directions.\n",
    "\n",
    "When we are talking about **Oversampling**, we have our minority class in mind. From this class, we are taking the data and then copying them again and again, until the minority and majority class are equal (or more  comparable) in the number of records.  \n",
    "***Pros***:    The data is no longer imbalanced and detects minority class with higher rates  \n",
    "***Cons***:    Our model might be overfitted for the minority class and not be able to predict it on slightly different data 🥲\n",
    "\n",
    "**Undersampling** is the polar opposite of oversampling and we perform it on the majority class. In this case, we take a sample of the size of minority class (or in the proportions we demand). This way our training dataset is usually smaller than initially, but is now balanced.  \n",
    "***Pros***:    The data is no longer imbalanced and detects minority class with higher rates    \n",
    "***Cons***:    Our model might be underfittet for the majority class and miss out on some additional patterns in the data.\n",
    "\n",
    "Whichever of these we choose, we need to remember about proper validation and testing on not-sampled data, so that we can compare if our sampling raised the predictability of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7665272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with importing our methods\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler \n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "from collections import Counter # cool function \n",
    "\n",
    "# and prepare a basic train test split for titanic\n",
    "y = titanic.pop('survived')\n",
    "X = titanic\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify=y \n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f9d07b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 339, 1: 339})\n"
     ]
    }
   ],
   "source": [
    "# using oversampler on minority class in titanic\n",
    "ros = RandomOverSampler(random_state=42, # we want it to always be the same with random_state\n",
    "                        sampling_strategy=1 # 1 will give us 1:1 proportion, but we can choose decimals as well\n",
    "                       )\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train) # let's resample\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_train_ros)) # check the sizes of our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b37c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 232, 1: 232})\n"
     ]
    }
   ],
   "source": [
    "# now undersampler will be exactly same in contruction\n",
    "rus = RandomUnderSampler(random_state=42, \n",
    "                         sampling_strategy=1)\n",
    "\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_train_rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99609e5",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "SMOTE, also known as Synthetic Minority Oversampling Technique, is a form of oversampling our minority class data. Differently to the random oversampling, instead of multipying the underrepresented data, SMOTE will generate new, similar records for it. It takes a similar algoritm to k-nearest neighbours and depending on the k_neighbors number it will create a grid among the nearest datapoint and interpolate to generate new, artifitial points (records).   \n",
    "![SMOTE](https://miro.medium.com/v2/resize:fit:734/1*yRumRhn89acByodBz0H7oA.png)\n",
    "\n",
    "\n",
    "***Pros***:    The data is no longer imbalanced and we didn't lose majority records nor multiplied the minority ones    \n",
    "***Cons***:    Our model is learning on synthetical data; SMOTE might also have problems with higher dimentions and many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f537559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 339, 1: 339})\n"
     ]
    }
   ],
   "source": [
    "# let's use SMOTE on our titanic dataset\n",
    "sm = SMOTE(random_state=42,\n",
    "           sampling_strategy='auto',\n",
    "           k_neighbors=4 # choosing the amount od neighbours, 5 is default\n",
    "          )\n",
    "\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_train_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b93c8",
   "metadata": {},
   "source": [
    "### TomekLinks\n",
    "Tomek Links is an alternative to random undersampling, as it does not remove random records from the majority class, but it removes the records that are the closest to the minority class records. It finds pairs of records from opposite classes by proximity and removes the record of majority class in the pair.\n",
    "![TomekLinks](https://mlwhiz.com/images/imbal/1.png)\n",
    "\n",
    "***Pros***:    The data is no longer imbalanced and it helps with decision boundaries.     \n",
    "***Cons***:    Our model might not learn subtleties of the true decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c354dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 296, 1: 232})\n"
     ]
    }
   ],
   "source": [
    "# let's use SMOTE on our titanic dataset\n",
    "tl = TomekLinks(sampling_strategy='auto')\n",
    "\n",
    "X_train_tl, y_train_tl = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_train_tl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20331896",
   "metadata": {},
   "source": [
    "## Balancing the dataset with multiclasses\n",
    "While balancing binary classes of majority and minority is quite easy, with multiclasses we might have some troubles when we want to classify with more than two classes. It's harder not to lose patterns, it's also more difficult to keep most of the training data and it's specificts. \n",
    "\n",
    "While we can use over- or undersampling with it's disadvantages, methods like SMOTE might also be not-ideal. With two variables it's easier to differentiate the patterns and boundaries, so SMOTE or TomekLinks will be good for that, however with multiclasses using these methods might cause class overlapping, which will actually worsen our model. Hence we must be very careful when deciding on methods to choose here.\n",
    "\n",
    "And of course, we have ***alternatives*** ~.\n",
    "\n",
    "Instead of using resampling methods, we might use model-level methods. \n",
    "For models these might include:\n",
    "- **Updating the loss function**, which penalizes the wrong classifications of the minority class more than wrong classification of majority classes - it forces the model to treat specific classes with more weight than others;\n",
    "- **Selecting appropriate algorithms**, that do well with imbalanced data, e.g. Gradient boosting trees (Decision Trees, XGBoost, Catboost), Random Forest or Ensemble methods (like AdaBoost or Bagging) - these models usually contain some kind of weights hyperparameters;\n",
    "- **Combining model with small over- or undersampling**, to better the position of minority class(es), but not evening it out too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68df6e5",
   "metadata": {},
   "source": [
    "## Information Value (IV) and Weight of Evidence (WoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e176517",
   "metadata": {},
   "source": [
    "*To be explained in the future*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
