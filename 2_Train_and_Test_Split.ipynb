{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115e48f1",
   "metadata": {},
   "source": [
    "# Train and Test Data Split\n",
    "In this notebook we will work on the train and test data split. It might look simple at first, what can be so difficult with assigning the train and test data, right? However, as we say, the further you go into forest, the more trees there are.\n",
    "\n",
    "When it comes to splitting the data we need to remember about a few rules that will allow us to create better models and allow for more precise model evaluation. \n",
    "\n",
    "**Basically, why do we split the data into train and test groups?**  \n",
    "In theory - we don't have to do it to create a model. We just need to feed the model with the independent data (our **X** values, all our features) and dependent data (our **y**, our desired outcomes) and our model will learn how to predict the y for the data we input later. But will it be a good model? Probably not. Will we know how good or bad it is? Also, probably not.   \n",
    "\n",
    "If we however divide the data into train and test datasets, we will, as the names suggest, train the model on the training data and then do some testing on the test data. Then we can compare the outcome on the test data with the actuall results and therefore evaluate our model. If the model does poorly on the predictions, it's a signal for us that we might want to tweek it a bit, change the parameters or do some more feature engineering so that we get a better result. We can also see, that the model is under- or overfitted and therefore it will make mistake on new data. \n",
    "\n",
    "In this notebook we will take a more theoretical look at the data splitting and some of the issues that come either from the splitting, but also take a look at the most common issues that might pop up at this point of machine learning journey:\n",
    "1. Using a population or sample data\n",
    "2. Basic train/test split\n",
    "3. Validation methods\n",
    "4. Dealing with imbalanced data\n",
    "5. Information Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686509b",
   "metadata": {},
   "source": [
    "## Using population or sample data\n",
    "The dataset we are using could also be called our population of data. Depending on the size of this dataset we might notice, that the population is considerably large (to consider if we have more than few hundred thousand records) and takes up a lot of memory, and therefore leads to longer modeling, especially when we are just experimenting, searching for correct model or trying to optimize the results. In such scenario we might consider using a sample of the data instead of the whole populations. \n",
    "\n",
    "|  | **Advantages** | **Disadvantages** |\n",
    "|:---|:---:|:---:|\n",
    "| **Population** | Represents the tendencies for the whole population | Takes more time to be processed, <br>Assumes that we have all the data, <br> Might be imbalanced for minority classes |\n",
    "| **Sample** | Is faster to process and model, <br>Might help with imbalanced data | Provides a generalization of the populations tendencies, <br>Might be bias or wrongly sampled |\n",
    "\n",
    "Choosing if we want to work on a sample or populations depends on different factors, dataset size, resources and time we have to do the task. If we want do use a sample, we can go a few different routes to achieve the desired, best sample for our task:\n",
    "- **Simple random sample** - which will create a random sample of the size we desire from our dataset. These data will we totally random and might miss on some patterns, therefore choosing the right size is very importand here, as well as comparing the sample to population for similarities;\n",
    "- **Stratified sample** - in which we divide the population into subgroups for the unique values (layers) of a feature and draw a sample from it. It ensures that every layer is represented, even it the proportions are slightly off.\n",
    "- **Proportional sample** - in which we are trying to save the proportions of the of the given feature;\n",
    "- **Systematic sample** - in which we take sample given the some intervals. We use a random starting point and then take n record in steps of our intervals. This works best if the data is sorted by some feature.\n",
    "\n",
    "> For our machine learning in these notes we do not need to take a sample, the dataset has only ~50k records and therefore is quite small, but we will draw the explained samples for this tutorial :smile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "414a8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import our libraries and read in our dataset that we preprocessed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# read our dataset for this notebook\n",
    "df = pd.read_csv('Diamonds_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4ed934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple random sample\n",
    "\n",
    "random_sample = df.sample(n=5000, # here we input the size of our sample\n",
    "                         random_state=42 # random state ensures that each time we draw, the sample will remain the same\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e74e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportional sample on clarity\n",
    "clarity_dict = {0: 'I1', 1: 'IF', 2: 'SI1', 3: 'SI2', 4: 'VS1', 5: 'VS2', 6: 'VVS1', 7: 'VVS2'} #as reminder\n",
    "\n",
    "# let's set up the weights \n",
    "# if we want the data to be splitted exactly, all should have the same weight assigned\n",
    "weights = {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1} \n",
    "\n",
    "proportional_sample = df.sample(n=5000, \n",
    "                         random_state=42,\n",
    "                         weights=df['clarity'].map(weights) # assigning weights\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c4fcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sample on cut\n",
    "\n",
    "proportion = 0.1  # Sample size as proportion to the population, here 10%\n",
    "\n",
    "stratified_sample = df.groupby('cut').apply(lambda x: x.sample(frac=proportion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7822fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# systematic sample with sorted cut\n",
    "df1 = df.sort_values(by='cut') # we need to sort and I don't want to overwrite my df\n",
    "\n",
    "# the following part can be done in one line as well\n",
    "proportion = 0.1\n",
    "population_size = len(df1)\n",
    "sample_size = int(population_size * proportion) \n",
    "step = population_size//sample_size\n",
    "\n",
    "# assign first draft\n",
    "first_index = np.random.randint(0, sample_size) \n",
    "\n",
    "systematic_sample = df1[first_index::step].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1aabd",
   "metadata": {},
   "source": [
    "## Basic train and test split\n",
    "Now that we decided whether we want to work on the population or sample, we can split our data for our machine learning model. As mentioned before, we do it so that we can check how well or bad our model is behaving. In many situations we will be spliting not only into training and testing sets, we will also create a validation set for hyperparameters tuning, but more on that later.\n",
    "\n",
    "Most often, we will split our data into 70-80 % for training and 20-30% for testing (and validation). For larger datasets we can split it more deliberately, for example 50/50 or 90/10, but we can dive into that the more we understand and tune our model. \n",
    "\n",
    "As the names sugest, we use our **training set** to train our model, providing it with the features/parameters values X and the expected values y. The **testing set** serves as, obviously, a test, in which we compare the model prediction with the expected, correct values. The more our model is correct in it's estimation, the better. \n",
    "\n",
    "> **In this notebook we will be using cut as the dependent value.**  If we were modeling at this point, we would have to scale the price value before putting it into the model, as it is widely spread out compared to the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48e3d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import our needed methods\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc117cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now assign the dependent and independed variables\n",
    "y = df.pop('cut') #dependent variable\n",
    "X = df # independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d56101",
   "metadata": {},
   "source": [
    "Of course for the X/y split there are tons of methods, but this is the one I like right now. If we are choosing only a few columns from the whole dataset for prediction, this code could look like this:\n",
    "```python\n",
    "X = df[['cut', 'depth', 'cube', 'clarity']]\n",
    "y = df['price']\n",
    "```\n",
    "\n",
    "We could also create pipelines that will automaticly assign the classes/expected values. This might come in handy when updating and already existing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30ec6ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43152, 12) (10788, 12)\n",
      "(43152,) (10788,)\n"
     ]
    }
   ],
   "source": [
    "# now let's split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, # our variables\n",
    "                                                   test_size=0.2, # proportion of the test size\n",
    "                                                   random_state=42, # so it is always divides same way\n",
    "                                                   stratify=y # for classification, so we get equal number of classes\n",
    "                                                   )\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82bbaa",
   "metadata": {},
   "source": [
    "## Validation and it's methods\n",
    "In the previous part, I mentioned that we can also distinguish a validation set. But before that, let's consider, what actually is validation and why we need it, when creating a machine learning model. \n",
    "\n",
    "If we were to split our data into train-validation-test, the validation set would be there to evaluate our model. Sounds too similar to what testing set does? The difference we make in here is that we keep the testing set until the very end, so that we could evaluate the model on data it hasn't seen before. Machine learning models tend to learn the provided data *by heart*, so to speak, therefore it's good to evaluate on data it has never seen before. The validation set is there, so that we can evaluate the model many times before calling it perfected (or good enough) and stop tuning the hyperparameters. Then, if the results are same or close on the testing set, we are good to go with deploying our model.\n",
    "\n",
    "There is also an alternative for validation set called **cross-validation**. It is useful, since if we were to use a validation set, we would have to sacrifice approximatel 10-20% of our data, that could be used for training. With cross-validation, the data can stay in the training set and we will have a way to validate the model. \n",
    "\n",
    "Cross-validation also serves as a controlling step, that will lessen the likelihood of model ovefitting. By using cross-validation, the data we feed into our model will be given in different states, shuffled and therefore our model will be more robust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca58f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic way with validation set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify=y \n",
    "                                                   )\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, # we use train sets in here!\n",
    "                                                  test_size=0.25,  # remember that X_train = 0.8 of X\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y_train\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2e3fe",
   "metadata": {},
   "source": [
    "Now with that we would:\n",
    "- use training set to train the model;\n",
    "- use validation set to validate the model and then tune the hyperparameters to see, if we can create a better outcome;\n",
    "- use the test set to test the final model after validation.\n",
    "\n",
    "However this way we are training the model on only **60%** of data. Therefore we call for the cross-validation method to the rescue.\n",
    "\n",
    "What the most of cross-validations do is they create a desired number of folds of train/test sets, there test data don't overlap in different folds. With these folds, we can train our model for the number of folds and take the average score, on which we are evaluating the model (more on the evaluation in different notebook!). When we are training the model with cross-validation, we do it in a loop.\n",
    "\n",
    "There are quite a few methods on cross-validation from Sklearn and we will take a few into consideration here, with honorable mentions as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09593b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the cross-validation methods\n",
    "from sklearn.model_selection import (LeaveOneOut, \n",
    "                                     KFold, \n",
    "                                     StratifiedKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1d526",
   "metadata": {},
   "source": [
    "### Leave-One-Out\n",
    "Leave-One-Out is one of the cross-validation methods, that can be used to teach a ML model. In each loop, the alghoritm will leave one record out and use it for evaluation, hence the name.  \n",
    "![LOOCV](https://www.researchgate.net/publication/344613547/figure/fig1/AS:1023136136978433@1620946072240/Schematic-representation-of-the-leave-one-out-cross-validation-LOOCV-method.ppm)\n",
    "\n",
    "LOO cross-validation is however very computationally expensive, as it basicly loops through the whole given dataset. It is therefore not recommended as a validation method for larger datasets, as it might simply take to long to be computed. LOO is a great method for smaller datasets (up to few thousand records) and might be great solution for fighting the overfitting, as it can be very precise.\n",
    "> In this notebook I will code the LOO for the diamonds dataset but won't run it, since it is a larger dataset.\n",
    "\n",
    "Alternatively to LOO, there is another cross-validation method called `LeavePOut()`, which allows us to define the number of records left out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12775fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv = LeaveOneOut()\n",
    "\n",
    "for train, test in loocv.split(X, y): # this loop provides us with the indexes of train and test rows\n",
    "    X_train, y_train = X.iloc[train], y.iloc[train] # assign the train spilt\n",
    "    X_test, y_test = X.iloc[test], y.iloc[test] # assign the train split\n",
    "    print(X_train.shape, X_test.shape) # here we will follow up with model fitting in next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13ee71",
   "metadata": {},
   "source": [
    "### KFold\n",
    "KFold cross-validation method might be the most popular one, as it's much faster and computationally cheaper. Similarely to Leave-One-Out, in each loop it takes a piece of the data for testing and does it for the k-number of folds. The testing data does not overlap and is evenly distributed in all the folds.\n",
    "![KFold](https://i0.wp.com/sqlrelease.com/wp-content/uploads/2021/07/K-fold-cross-validation-1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9570a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n"
     ]
    }
   ],
   "source": [
    "kfoldcv = KFold(n_splits=5, # number of folds, 5 is default\n",
    "               shuffle=True, # if true, the data is shuffled at the beggining\n",
    "               random_state=42 # ensures that when we use it again we will get same results\n",
    "               )\n",
    "\n",
    "# and the loop is the same for each cross-validation\n",
    "for train, test in kfoldcv.split(X, y):\n",
    "    X_train, y_train = X.iloc[train], y.iloc[train]\n",
    "    X_test, y_test = X.iloc[test], y.iloc[test]\n",
    "    print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf7bca",
   "metadata": {},
   "source": [
    "### Stratified KFold\n",
    "This cross-validation is a variation of KFold that returns stratified folds (we talked about stratification earlier). The folds are made by preserving the percentage of samples for each category. This is a good cross-validation for imbalanced data, as it ensures the presence of each category in every training and testing fold.\n",
    "![Stratified KFold](https://i.stack.imgur.com/XJZve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e07c24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n",
      "(43152, 12) (10788, 12)\n"
     ]
    }
   ],
   "source": [
    "strkfoldcv = KFold(n_splits=5,\n",
    "               shuffle=True,\n",
    "               random_state=42 \n",
    "               )\n",
    "\n",
    "# and the loop is the same for each cross-validation\n",
    "for train, test in strkfoldcv.split(X, y):\n",
    "    X_train, y_train = X.iloc[train], y.iloc[train]\n",
    "    X_test, y_test = X.iloc[test], y.iloc[test]\n",
    "    print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee987d8",
   "metadata": {},
   "source": [
    "### Groups\n",
    "With cross-validation and generally, in data analytics and ML as well, we sometimes come across the usage of *groups* in our datasets. What are these and how do we use it? Well, I'm glad I asked.\n",
    "\n",
    "In regards to our dependent and independent variables, groups won't fit into any of these. The groups are there in the dataset as the sources of the records. \n",
    "\n",
    "For example, let's assume that for the dataset we are working with, diamonds, each unique diamond color comes from a different mine. And therefore, if we are doing a classification for the cut of the diamond, we can later see, where do the best cuts come frome and where the cuts are so-so (this is a made up example to the used dataset, as unfortunately there are not groups in it).  \n",
    "Another example could be that we are given the patients data from few different hospitals and need to determine, if they are going to have a heart attack or not. The group in here would be the hospitals, from which the informations about the patient come. If one of the hospital has high level of heart attacks, maybe they do not take good care of their patients? Or maybe this hospital is located in an unhealthy area? Or, perhaps, they specialise in heart diseases? \n",
    "\n",
    "Either way, different sources of information/records might cause the model to be to sensitive or not sensitive enough for our prediction. Like in the example given above with the hospitals, if we give the model training data from \"normal\" hospitals and then ask the same model to predict the heart attacks for patients in the heart-specializing hospital, we won't get good predictions. This might also go the other way round, as if we would train the model with data from mainly heart-spetializing hospitals and then ask it to predict heart attacks for a childrens hospital, where the rates are much lower.\n",
    "\n",
    "Importatant to remember here is that groups aren't a feature in the sense that we train our model with them. We most likely will come across the groups when creating a larger, general models, in which we would like to avoid biases and make mistakes based on the sources of our data, if they come from a source different than the ones we trained our model with.\n",
    "\n",
    "Why mentioning it? Well, there are cross-validation methods that allow us to validated our train and test data with considerations of the groups as well.\n",
    "\n",
    "### Honorable mentions\n",
    "As I explained, how the groups work in data above, we can move on to a few different cross-validation methods that are involving the usage of groups.\n",
    "- **GroupKFold** - which ensures that for each fold our model is validated on data from a different group(s) than the ones it was learned on;\n",
    "- **StratifiedKFold** - which is a mixture of it's name, it takes data from each group to be trained and tested on, while preserving the percentage of samples for each group;\n",
    "- **LeaveOneGroupOut** - where for each iteration, different group is left out for validation;\n",
    "- **PredefinedSplit** - in which we predefine the schema for the spilt;\n",
    "- **TimeSeriesSplit** - which is a special cross-validation method for time-series forecasting, which I will coved later on, in time series notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4450106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.5.1.tar.gz (356 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.3/356.3 kB\u001b[0m \u001b[31m402.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.5.1-py2.py3-none-any.whl size=351211 sha256=734f0c751d4cf2cf938839a89e3ebead8142495655451b7cdb22ffe1c09b8b63\n",
      "  Stored in directory: /Users/miladziekanowska/Library/Caches/pip/wheels/eb/20/3c/5fd1add8fe5078a1fccc478726dfe8558eca5bfa5ba5b28405\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036d435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
