{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1273526",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe286a",
   "metadata": {},
   "source": [
    "One of the most important part when it comes to machine learning is feature engineering and data preprocessing. Why destinguishing these? Feature engineering is way larger than data preprocessing we will be discussing here. While feature engineering might include creating and deleting certain features, filling the missing values and dealing with outliers to name a few, data preprocessing will tranform the final features in some mathematical way, so that these will be easier to digest for our model.\n",
    "\n",
    "Some of the models, like K-Nearest Neighbours or SVM models require the data to be standarized, while models like Decision Trees or Random Forest could work on just cleaned data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5953678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the tools\n",
    "import numpy as np # for linear algebra and some additional transformations\n",
    "import pandas as pd # for some examples\n",
    "import seaborn as sns # for dataset\n",
    "\n",
    "# preprocessing tools from sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MaxAbsScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fbf045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the dataset for examples\n",
    "df = sns.load_dataset(\"diamonds\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b6a2f",
   "metadata": {},
   "source": [
    "## Data leakage\n",
    "Data leakage is an issue that might occur when we are creating our machine learning models, where we feed our model with data, that have some encoding of our target or the same/other feature, that indicates the spread of these data, values etc. \n",
    "\n",
    "But why worry about data leakage? Depending on the kind of data leakage, it might be harmless to our model, but it can lead to overfitting the model and for it to memorise the predictions. It might also lead to exclusion or diminishing the importance of some features, that might bring up some additional patterns in data. \n",
    "\n",
    "We might differentiate a few types of data leakages:\n",
    "- **Target leakage** - in which we leak the predicted value into our training and testing data. This might a result of:\n",
    "    - Badly done target encoding;\n",
    "    - Wrong recognision of provided features during EDA - some of the features might be results of our predictions, not features leading to our prediction;\n",
    "- **Train-Test contamination** - where we preprocess on the whole dataset. In other words - we do not want the model to learn anything from the testing and validation sets (more on this split in next notebook). Otherwise, once we feed brand new informations, we will get worse results. This might be:\n",
    "    - Scaling numeric data on the whole dataset; \n",
    "    - Filling the NaN values with any kind of center metrics (e.g. mean, median);\n",
    "    - Target encoding using mean/aggregation values;\n",
    "    - Any feature engineering that uses whole dataset aggregation.\n",
    "    \n",
    "While any data scaling must be done in splits, separately for train and test splits, **one-hot** and **label encoding** of string valeus can be done at any point (either before or right after the split).\n",
    "\n",
    "### How to prevent data leakage?\n",
    "To avoid data leakage and ensure our model will be working as similar as possible on any given data (whether we will compare it on the training or brand new data), we should follow, apply or at least check the following things:\n",
    "1. Do a manual check on our data - deep EDA and understanding is needed. We should check and consider all features that are highly correlated with our target (if these are a cause or a result?) or if these are in a way, co-dependent.\n",
    "2. Splitting our data into train and test (and validation) sets and using cross-validation, to check how the model is behaving on different set-ups - if the scores in cross-validation are very different each time, we might be dealing with data leakage;\n",
    "3. Minimizing the features amount going into the model, by using feature selection and extraction techniques (more on them in third notebook);\n",
    "4. Using **pipelines**, to automate the process and scale the accurate data in correct time and not in advance. And also pipelines are really handy!\n",
    "\n",
    "> **NOTE!** In the following part, we will not care for data leakage in preprocessing of the data, as these are only examples. The appropriate way, in which we prevent the data leakage will be shown in the third notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4371",
   "metadata": {},
   "source": [
    "## String encoding preprocessing\n",
    "Machine Learning models won't be able to process string or object data types. Therefore we need to transform our string or object data into numeric representation. I already covered this using different methods in data analytics on my github.\n",
    "\n",
    "Scikit-Learn provides a few options for string data encoding as well, which are for one-hot and label encoding. For this notebook I will work on two different methods.\n",
    "\n",
    "### One-hot encoding with sklearn\n",
    "As written in the mentioned notes, one-hot encoding creates additional columns in which the label data are encoded in 0 (is not) and 1 (is). This creates an additional, often sparse table, attached at the end of our dataset. \n",
    "\n",
    "It is good to use with binary classification or labeling (Yes or No questions) and when there are not that many unique values in the string/object column. The more labels there are, the larger our table gets and therefore takes up more space, which leads to longer modeling. The best way to use it is when there are up to 10-ish unique labels we want to encode for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995e507e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['D', 'E', 'F', 'G', 'H', 'I', 'J'], dtype=object)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How we use the OneHotEncoder from sklearn\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', # this way if something doesn't have a value there won't be errors\n",
    "                       drop='first',# this way we will have one less column,\n",
    "                       dtype='uint8' # this would be the smallest in terms of memory\n",
    "                      )\n",
    "\n",
    "X = onehot.fit_transform(df[['color']]).toarray() # we need to put this into array\n",
    "onehot.categories_ # so we see the categories - remember, we dropped the first one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad66e5",
   "metadata": {},
   "source": [
    "Now we have created an array with 0 and 1, we can attach it to the original dataframe. For that however we need to create new columns and know the labels for each. That's why above I've used he `onehot.categories_` to know the order in which these were encoded. But we can use slicing as well 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfa7952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat    cut color clarity  depth  table  price     x     y     z  E  F  G  \\\n",
       "0   0.23  Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43  1  0  0   \n",
       "\n",
       "   H  I  J  \n",
       "0  0  0  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[onehot.categories_[0][1:]] = X # this way we add this to the df\n",
    "# we could also immediately assign the one-hot columns to the table\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be3b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we won't be using the cut feature anymore, we can drop it\n",
    "df = df.drop(['color'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ffaca",
   "metadata": {},
   "source": [
    "Now, we miht wonder, what is the difference between OneHotEncoder, and `pandas.get_dummies()` method, which is also used for one-hot encoding of categorical data.\n",
    "- **pandas.get_dummies()** creates an additional dataframe, which we can immediatly append into the original dataframe;\n",
    "- **OneHotEncoder()** creates and array and is useful when creating pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e5024",
   "metadata": {},
   "source": [
    "### Label encoding with sklearn\n",
    "While there are at leadst a few ways to create label encoding using pandas and standard python, there is no dedicated method for this operation. However, it is provided in sklearn library with `LabelEncoder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0424c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  cut clarity  depth  table  price     x     y     z  E  F  G  H  I  J\n",
       "0   0.23    2     SI2   61.5   55.0    326  3.95  3.98  2.43  1  0  0  0  0  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to use LabelEncoder()\n",
    "labeler = LabelEncoder() # here we do not need to define anything inside\n",
    "\n",
    "df['cut'] = labeler.fit_transform(df['cut']) # notice that there is no double square bracket\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a562513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Fair', 1: 'Good', 2: 'Ideal', 3: 'Premium', 4: 'Very Good'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we would like to check what are the representations of each category\n",
    "cut_labels = dict(zip(labeler.transform(labeler.classes_), labeler.classes_)) # create a dictionary\n",
    "cut_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06a5732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'I1', 1: 'IF', 2: 'SI1', 3: 'SI2', 4: 'VS1', 5: 'VS2', 6: 'VVS1', 7: 'VVS2'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  cut  clarity  depth  table  price     x     y     z  E  F  G  H  I  \\\n",
       "0   0.23    2        3   61.5   55.0    326  3.95  3.98  2.43  1  0  0  0  0   \n",
       "\n",
       "   J  \n",
       "0  0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we will be using this dataset for following, we might as well encode the clarity\n",
    "labeler1 = LabelEncoder()\n",
    "\n",
    "# encode the data\n",
    "df['clarity'] = labeler1.fit_transform(df['clarity']) \n",
    "\n",
    "# create a dictionary\n",
    "clarity_labels = dict(zip(labeler1.transform(labeler1.classes_), labeler1.classes_))\n",
    "print(clarity_labels)\n",
    "\n",
    "#display changes\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03837f7",
   "metadata": {},
   "source": [
    "## Numeric scaling preprocessing\n",
    "Now that we handled the some of the string data, we need to talk about the numeric data and why it is important to sometimes, if not always, scale them for machine learning algorithms. Many of them are using distances or gradient. If the data are widely spread out within one categoty and narrow within another, this might lead to minimazing the role of these narrowly spead data.\n",
    "\n",
    "To avoid this, we often do some data scaling on our data, so that it is in one scale - therefore all features have the same starting point to affect the target data. Also, the models that are using distances between data will need slightly less memory to process the data. A few models that need data scaling are KNN or SVM. Also, Neural Networks require scaling to work correctly. \n",
    "\n",
    "It is not nessesary to scale data we encoded in the previous part, however if we label-encode a lot of data, we might want to scale it a bit, so that instead of range(0, 20) the range would be (0, 4) with float values.\n",
    "\n",
    "### Standars Scaler from Sklearn\n",
    "Standard Scaler is one of the most commonly used scaling algorithm. It takes the data we want to scale and assigns it their positions depending on the mean and standard deviation. The mean of the data is in value 0, values smaller than the mean - are below 0, and larger than the mean - will be above 0, in the equivalent value, depending on the standard deviation. The most outstanding values (larger than 3 standard deviations on both ways) are the outliers. We could also switch it up and use median instead of mean and use variance or median absolute deviation.\n",
    "\n",
    "Standard scaler does alter the distribution of data from the original into normal distibution! So it is best to use StandardScaler on data that is more or less normally distributed.\n",
    "\n",
    "> **Attention!**  \n",
    "> For this notebook we will scale each column differently. In real life scenario we would rather scale all the data in the same way, sometimes stack a few scaling methods on top of each other, rather than encoding numerical values in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f74371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because I want to save the encoding for future sets:\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a38450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.174092</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  cut  clarity     depth  table  price     x     y     z  E  F  G  H  \\\n",
       "0   0.23    2        3 -0.174092   55.0    326  3.95  3.98  2.43  1  0  0  0   \n",
       "\n",
       "   I  J  \n",
       "0  0  0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use standard scaler on the depth\n",
    "scaler = StandardScaler(with_mean=True, # here we can change to median\n",
    "                       with_std=True #here we can change to other deviation value\n",
    "                       )\n",
    "\n",
    "X = scaler.fit_transform(df1[['depth']]) # transformation\n",
    "df1['depth'] = X # assigning the value\n",
    "\n",
    "# if we are scaling the whole dataset, we can omitt the assignment and create X_scaled for whole dataframe\n",
    "\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de22b62",
   "metadata": {},
   "source": [
    "### Maximum Absolute Scaler from Sklearn\n",
    "MaxAbsScaler is another common scaling technique. It takes the absolute values from the data (especially important to remember when we are dealing with positive and negative values) and uses the maximum value as the reference. Then, back in the original setting, it divides all the values in the set by the maximum absolute value. If the maximum absolute value is originally negative - then it will have the value of -1, and if the maximum absolute value is positive, it will end up with the value of 1. All the remaining values will then fit within the range of (-1, 1).\n",
    "\n",
    "MaxAbsScaler does not alter the distribution of data from the original distribution. It is good to use when we want to preserve the proportions among the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c09764f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.174092</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  cut  clarity     depth     table  price     x     y     z  E  F  G  \\\n",
       "0   0.23    2        3 -0.174092  0.578947    326  3.95  3.98  2.43  1  0  0   \n",
       "\n",
       "   H  I  J  \n",
       "0  0  0  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's use the MaxAbsScaler on table\n",
    "maxabs = MaxAbsScaler() # no need to define anything inside\n",
    "\n",
    "X = maxabs.fit_transform(df1[['table']])\n",
    "df1['table'] = X # assigning the value\n",
    "\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d79fa",
   "metadata": {},
   "source": [
    "### Robust Scaler from Sklearn\n",
    "RobustScaler is yet another popular scaling method, which is commonly used then dealing with outliers in the changed data (we don't have them in our current dataset, but don't tell them about it). Robust scaler is in a way similar to StandardScaler, however it uses differen central and deviation values. For the transformation, it subtracts the median from the value and then divides it by the IQR. \n",
    "\n",
    "RobustScaler keeps the distribution more or less the same as the original (we say that is does not alter it, since the changes are minor). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b68f68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.734375</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.174092</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      carat  cut  clarity     depth     table  price     x     y     z  E  F  \\\n",
       "0 -0.734375    2        3 -0.174092  0.578947    326  3.95  3.98  2.43  1  0   \n",
       "\n",
       "   G  H  I  J  \n",
       "0  0  0  0  0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the Robust Scaler let's use the carat column\n",
    "robust = RobustScaler() # we can set up a few things in here, but I won't\n",
    "\n",
    "X = robust.fit_transform(df1[['carat']])\n",
    "df1['carat'] = X # assigning the value\n",
    "\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20086b",
   "metadata": {},
   "source": [
    "### Min Max Scaler from Sklearn\n",
    "MinMaxScaler is also one of the key scaling methods a data scientist needs to learn. It is quite common due to it's narrow range <0,1> and is simple to understand. What is does, is that is takes the value from out column, subtracts the minimum value of the column and divides by the difference between the maximum value and the minimum value of the column. \n",
    "\n",
    "MinMaxScaler does not alter the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a4c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the minmaxscaler, let's create a new column = cube (we could also scale the paramaters all in one way)\n",
    "df['cube'] = df['x'] * df['y'] * df['z']\n",
    "df = df.drop(['x', 'y', 'z'], axis=1)\n",
    "df1['cube'] = df['cube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4cd4c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>cube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.734375</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.174092</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      carat  cut  clarity     depth     table  price     x     y     z  E  F  \\\n",
       "0 -0.734375    2        3 -0.174092  0.578947    326  3.95  3.98  2.43  1  0   \n",
       "\n",
       "   G  H  I  J      cube  \n",
       "0  0  0  0  0  0.009947  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = MinMaxScaler(feature_range=(0,1), # this way we can assign the range ourselves!\n",
    "                     )\n",
    "\n",
    "X = minmax.fit_transform(df1[['cube']])\n",
    "df1['cube'] = X # assigning the value\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a5d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point our dataset is ready to be used in the following examples, so we can move on with it\n",
    "df.to_csv('Diamonds_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd03ca9",
   "metadata": {},
   "source": [
    "### Honorable metions\n",
    "There are of course different encoding and scaling methods, that have not been discussed here, since we ran out ouf features to scale :lol:\n",
    "\n",
    "For the encoding, the most common method besides these two would be `target encoding`, which in itself has a few methods. It encodes our categorical data using different features' value(s'). It could however lead to data leakage, so as tempting as it is, we must encode these data thoughtfully.\n",
    "\n",
    "On the scaling, there are a few more worth mentioning, that might come in handy and perhaps I will extend this note in the future with the following scalers:\n",
    "- `sklearn.preprocessing.Normalizer`\n",
    "- `sklearn.preprocessing.PowerTransformer`\n",
    "- `sklearn.preprocessing.QuantileTransformer`\n",
    "- `np.log`\n",
    "- `scipy.stats transformations`\n",
    "- `feature-engine transformations`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
